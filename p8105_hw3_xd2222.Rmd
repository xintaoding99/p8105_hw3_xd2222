---
title: "p8105 homework 3"
author: "Xintao Ding"
date: "10/8/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Probelm 1
Exploring the dataset instacart
```{r explore instacart dataset}
library(tidyverse)
library(dplyr)
library(p8105.datasets)
data("instacart")


instacart %>% 
  summarize(n_asile = n_distinct(aisle))
  
most_asile <- instacart %>%
  count(aisle) %>% 
  arrange(desc(n))
most_asile
# There are 134 aisles, and the "fresh vegetables" aisle is where most products are ordered from, second comes after is the "fresh fruits" aisle


# Plotting 
most_asile %>% 
  filter(n > 10000) %>% 
  arrange(desc(n)) %>% 
  ggplot(aes(x = reorder(aisle, n), y = n)) + 
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Number of Items Ordered from Asile", x= "Aisle", y = "Number of Products") +
  coord_flip()

# Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”

instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle, product_name) %>% 
  summarize(number = n()) %>% 
  arrange(desc(number)) %>% 
  filter(min_rank(desc(number)) < 4) %>% 
  knitr::kable() 


# Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers 

instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = round(mean(order_hour_of_day),1)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) %>% 
  knitr::kable(col.names = c("Product Name", "Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))

```

## Problem 2
```{r explore BRFSS dataset }
library(p8105.datasets)
data("brfss_smart2010")
brfss <- brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  mutate(response = fct_relevel(response, c("Poor", "Fair", "Good", "Very good", "Excellent" )))

#2002 data
brfss %>% 
  filter(year == 2002) %>% 
  distinct(locationabbr, locationdesc) %>% 
  count(locationabbr) %>% 
  filter(n >= 7)

# 2010 data 
brfss %>% 
  filter(year == 2010) %>% 
  distinct(locationabbr, locationdesc) %>% 
  count(locationabbr) %>% 
  filter(n >= 7)
# In 2002, CT, FL, MA, NV, NJ, PA were observed at 7 or more locations. In 2010, CA, CO, FL, MA, MD, NC, NE,NJ, NY, OH were observed at 7 or more locations

# Spaghetti plotting to show mean data values within each state across years
brfss %>% 
  filter(response == "Excellent") %>% 
  select(year, state = locationabbr, locationdesc, data_value) %>% 
  group_by(year, state) %>% 
  summarize(mean_data_value = round(mean(data_value), digits = 1)) %>% 
  ggplot(aes(x = year, y = mean_data_value)) + 
  geom_line(aes(group= state, color = state)) +
  labs(title = "Average Data Value within Each State Across Years", x = "Year", y = "Average Data Value", catpion = "Data from brfss")
  

# Two panel plot showing 2006, 2010 distribution of data value for responses among locations in NY state
brfss %>% 
  filter(year %in% c(2006,2010), locationabbr == "NY") %>% 
  select(year, locationdesc, response, data_value) %>% 
  ggplot(aes(x=data_value , y=response, color=locationdesc)) +
  geom_point() +
  labs(title = "Data Value Across Responses Among Locations in NY", x = "Data Value", y = "Response", catpion = "data from brfss") +
  facet_grid(~year)
```

## Problem 3
```{r Accelerometer Data}
acc_data <- read_csv("./accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(activity_1:activity_1440,
    names_to = "activity", 
    values_to = "activity_count"
  ) %>% 
  mutate(day_type = case_when(
    day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday") ~ "weekday",
    day %in% c("Saturday", "Sunday") ~ "weekend", 
  ))


acc_data %>% 
  count() 

 
acc_data %>% 
  colnames()

# After tidying the data, there are 6 varialbes in this dataset: week, day_id, day, activity, activity_count, and day_type
# There are 50,400 observations in this dataset
```

